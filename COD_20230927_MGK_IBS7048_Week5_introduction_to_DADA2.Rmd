---
title: "COD_20230927_MGK_IBS7048_week5_introduction_to_DADA2"
author: "Minsik Kim"
date: "2023-09-27"
output:
        rmdformats::downcute:
        downcute_theme: "chaos"
code_folding: show
fig_width: 6
fig_height: 6
df_print: paged
editor_options: 
        chunk_output_type: inline
markdown: 
        wrap: 72
---
        
```{r warning=FALSE, message=FALSE, echo=FALSE, results='hide', setup}
#===============================================================================
#BTC.LineZero.Header.1.1.0
#===============================================================================
#R Markdown environment setup and reporting utility.
#===============================================================================
#RLB.Dependencies:
#   knitr, magrittr, pacman, rio, rmarkdown, rmdformats, tibble, yaml
#===============================================================================
#Input for document parameters, libraries, file paths, and options.
#=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=
knitr::opts_chunk$set(message=FALSE, warning = FALSE)

path_working <- "/Users/minsikkim/Dropbox (Personal)/Inha/5_Lectures/Advanced biostatistics/scripts/BTE3207_Advanced_Biostatistics/"
path_library <- "/Library/Frameworks/R.framework/Resources/library"
str_libraries <- c("tidyverse", "pacman", "yaml")


YAML_header <-
        '---
title: "COD_20230927_MGK_IBS7048_week5_introduction_to_DADA2"
author: "Minsik Kim"
date: "2032.09.27"
output:
    rmdformats::downcute:
        downcute_theme: "chaos"
        code_folding: hide
        fig_width: 6
        fig_height: 6
---'
seed <- "20230920"

#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
#Loads libraries, file paths, and other document options.
#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
FUN.LineZero.Boot <- function() {
        .libPaths(path_library)
        
        require(pacman)
        pacman::p_load(c("knitr", "rmarkdown", "rmdformats", "yaml"))
        
        knitr::opts_knit$set(root.dir = path_working)
        
        str_libraries |> unique() |> sort() -> str_libraries
        pacman::p_load(char = str_libraries)
        
        set.seed(seed)
}
FUN.LineZero.Boot()
#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
#Outputs R environment report.
#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
FUN.LineZero.Report <- function() {
        cat("Line Zero Environment:\n\n")
        paste("R:", pacman::p_version(), "\n") |> cat()
        cat("Libraries:\n")
        for (str_libraries in str_libraries) {
                paste(
                        "    ", str_libraries, ": ", pacman::p_version(package = str_libraries),
                        "\n", sep = ""
                ) |> cat()
        }
        paste("\nOperating System:", pacman::p_detectOS(), "\n") |> cat()
        paste("    Library Path:", path_library, "\n") |> cat()
        paste("    Working Path:", path_working, "\n") |> cat()
        paste("Seed:", seed, "\n\n") |> cat()
        cat("YAML Header:\n")
        cat(YAML_header)
}
FUN.LineZero.Report()


```

# DADA2

You can find the original DADA pipeline at 

https://benjjneb.github.io/dada2/

Meanwhile, I will add 

## Installing DADA2

Binaries for the current release version of DADA2 (1.26) are available from Bioconductor. Note that you must have R 4.2.0 or newer, and Bioconductor version 3.16, to install the most current release from Bioconductor.

If you are using R4.3 or higher, you should install dada2 version 3.17.

```{r}
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install(version = "3.17")


if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install("dada2", version = "3.17")
```

## Load DADA2 to your environment

Now, your DADA2 package was installed to your computer. That does not mean it is loaded to your working environment.

To double-check the loaded packages, you can run `sessionInfo()`.

```{r}
sessionInfo()

```

As you can see, DADA2 is not loaded. To load package, use `library()` function.

```{r}
library(dada2)


sessionInfo()

```

You can see the dada2 package was loaded.


# Opening multiple files for dada2 in R

Without pointing the exact path of a file that you want to play with, R will employ thing at your current working directory. 

`getwd()` will show you the current directory that you are working on.

```{r}
getwd()
```

`list.files()` function shows all the files in the current list.

```{r}
list.files()
```

You can also set the path of a folder of your interest for `list.files()`.

```{r}
list.files("/Users/minsikkim/Dropbox (Personal)/Inha/5_Lectures/Advanced metagenomics/IBS7048_dataset")


```
 
This is my directory with the example files.

*You can also navigate folders in R*. Try typing `list.files("/")` and press `tab`.

Anyway, I can set name this path as `path`, as it is too long.

```{r}
minsik_path = "/Users/minsikkim/Dropbox (Personal)/Inha/5_Lectures/Advanced metagenomics/IBS7048_dataset"

# you can also use `<-` instead of `=`.
```
 
This is my directory with the example files.

As now `path` is having the information of my location, I can substitute the long path name with `path` 

```{r}
list.files(minsik_path)
```


# Chooing files with pattern

From the list of files, we only want to select `.fastq` files. For that, I am going to make a list of files.

Since this file list is having forward and reverse reads per one sample set, we need to separate those files into two groups as well.

```{r}
list.files(minsik_path, pattern = "_R1_001")
```

Using `pattern = ` argument will make a subset of list of files including the pattern noted in the quotation mark `""`.

With another argument, `full.names = TRUE`, it will give you the list of full-path-names of the files with the specified pattern.

```{r}
list.files(minsik_path, pattern = "_R1_001", full.names = TRUE)

```

So, now what?

We are going to store their list of names of files, so that some other function can use that information for loading that data at once.

```{r}
forward_read_files <- list.files(minsik_path, pattern="_R1_001.fastq", full.names = TRUE)

reverse_read_files <- list.files(minsik_path, pattern="_R2_001.fastq", full.names = TRUE)


```

Meanwhile, they can be having different orders. So it would be helpful if we can sort the data right after loading it.

```{r}
library(tidyverse)
forward_read_files <- list.files(minsik_path, pattern="_R1_001.fastq", full.names = TRUE) %>% sort()

reverse_read_files <- list.files(minsik_path, pattern="_R2_001.fastq", full.names = TRUE) %>% sort()
```

# Manipulating characters

Plus, after processing the data, we need to get `1 sample name per 1 sample`. It can be extracted from either one of the forward and reverse reads. 

`basename()` removes all the higher path of file path.

```{r}
forward_read_files %>% 
        basename()
```

`str_split_fixed()` will separate all the charaters separated by the pattern specified in quotation mark `""` and generate a matrix of the separated character. The number of separated output will be set as you set. (you can think of the import wizard in Excel).


```{r}
forward_read_files %>% 
        basename() %>% 
        str_split_fixed(pattern = "_", n = 5)
```


if you choose 1st column from the data, using `.[,1]`, it wlll show the name of files after removing all the characters after the first underscore `_`.



```{r}
forward_read_files %>% 
        basename() %>% 
        str_split_fixed("_", 5) %>% 
        .[,1]
        
```

I am going to store this list of file names for future use.

```{r}
sample.names <- forward_read_files %>% 
        basename() %>% 
        str_split_fixed("_", 5) %>% 
        .[,1]
        
```

# Plotting 

DADA2 have a cool function called `plotQualityProfile()`, which will automatically plot QC scores by length of all the files that were listed. Such as,

If I want to plot the QC profile of the f
```{r}
forward_read_files
forward_read_files[1]

```


```{r}
plotQualityProfile(forward_read_files[1])
```

It will show the total reads, QC score, and the length of sequencing reads at the same time.

We can do this for multiple samples, by using

```{r}
plotQualityProfile(
        c(forward_read_files[1], forward_read_files[2])
)
```

or


```{r}


plotQualityProfile(forward_read_files[1:2])
```

or


```{r}
plotQualityProfile(forward_read_files[1:10])
```

Let's check reverse reads as well.


```{r}
plotQualityProfile(reverse_read_files[1:10])

plotQualityProfile(reverse_read_files[1])
```

As you can see, we have high quality forward reads but low quality reverse reads. These low quality reads need to be removed before mering these files into one complementary read file.

# Filter and trim

Before making filtered reads, let's set an location and names for them.

Using `paste()` function, we can manipulate charater variables like ties.


```{r}
sample.names

paste0(sample.names, "_sample")
```

Using paste0, we can create a list of new names at once.

```{r}
# Place filtered files in filtered/ subdirectory
filtered_forward_reads <- file.path(minsik_path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtered_reverse_reads <- file.path(minsik_path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))

minsik_path[1]
filtered_forward_reads[2]
filtered_forward_reads

```

We can also assign names of each element in that list, using `names()` function.

```{r}
names(filtered_forward_reads) <- sample.names
names(filtered_reverse_reads) <- sample.names
filtered_forward_reads
```


Now, we are using the dada2 filtering function.

`filterAndTrim()` have multiple options for filtering. 

1. the path of original file (forward)
2. the path of new file (forward)
3. the path of original file (reverse)
4. the path of new file (reverse)
5. truncLen: Reads after 160th bp showed lower QC scores. Lets remove them, from 160 to 250.
6. maxEE: Maximum error after truncation
7. compress=TRUE: The output files will be gzipped.
8 multithread=TRUE: Turn on this option if you are using Windows OS

The maxEE parameter sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores.


```{r}
out <- filterAndTrim(forward_read_files, #name of forward raw reads
                     filtered_forward_reads, #name of filtered forward reads 
                     reverse_read_files,
                     filtered_reverse_reads,
                     truncLen=c(250,150),# Reads after 160th bp showed lower QC scores. Lets remove them!
                     compress=TRUE,
                     #multithread=TRUE, # On Windows set multithread=FALSE
                     maxEE=c(2,2))
head(out)

```

You can see some reads were removed from the fastq file!

and they are now stored in a new directory.

```{r}
list.files(path = "/Users/minsikkim/Dropbox (Personal)/Inha/5_Lectures/Advanced metagenomics/IBS7048_dataset/filtered/")
```

# Error prediction

The DADA2 algorithm makes use of a parametric error model (err) and every amplicon dataset has a different set of error rates. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

```{r }
error_F <- learnErrors(filtered_forward_reads
                       #, multithread=TRUE #
                       )
```

```{r}
error_R <- learnErrors(filtered_reverse_reads
                       #, multithread=TRUE
                       )
```

We can double-chekc when the error is ocurring in our sequencing file, using `plotErrors()` function.

```{r}
plotErrors(error_F, nominalQ=TRUE)
```


Using this `predicted error` (which can be also called as `expected error`) in adjusting the actuall error happedn in our data set. In other words, errors that is higher than the expectation will be strongly filtered.

Use `dada()` function to remove errors and get **unique sequences**!


```{r}
forward_dada <- dada(filtered_forward_reads, err=error_F)
#multithread=TRUE for windows
```

```{r}
reverse_dada <- dada(filtered_reverse_reads, err=error_R)
#multithread=TRUE for windows
```

```{r}
forward_dada[[1]]
```

Now, we have `sequencing variants` after considering errors.

# Mering paired reads

Unique sequences can be merged into single, complementary reads (forward + reverse) using `mergePairs()` function.


```{r}
mergers <- mergePairs(forward_dada, 
                      filtered_forward_reads,
                      reverse_dada,
                      filtered_reverse_reads,
                      verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```

# Sequence table

Table of sequences

```{r}

seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```

20 files, 291 unique sequencing reads

Length of merged sequences

```{r}
table(nchar(getSequences(seqtab)))

```


# Remove chimeras

The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```

291-225 = 66 reads were chimeras.

```{r}
sum(seqtab.nochim)/sum(seqtab)
```

4% of merged readsa were chimeras.

# Track reads through the pipeline


As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline:

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out,
               sapply(forward_dada, getN),
               sapply(reverse_dada, getN), 
               sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```


# Assign taxonomy


It is common at this point, especially in 16S/18S/ITS amplicon sequencing, to assign taxonomy to the sequence variants. The DADA2 package provides a native implementation of the naive Bayesian classifier method for this purpose. The assignTaxonomy function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least minBoot bootstrap confidence.

We maintain formatted training fastas for the RDP training set, GreenGenes clustered at 97% identity, and the Silva reference database, and additional trainings fastas suitable for protists and certain specific environments have been contributed. For fungal taxonomy, the General Fasta release files from the UNITE ITS database can be used as is. To follow along, download the silva_nr_v132_train_set.fa.gz file, and place it in the directory with the fastq files.

```{r}

taxa <- assignTaxonomy(seqtab.nochim,
                       paste0(minsik_path, "/silva_nr99_v138.1_train_set.fa"),
                       multithread=TRUE)


```

```{r}
view(taxa)
```

Read count information (to compare their abundances between taxa within sample)

```{r}
view(seqtab.nochim)
```

Constructing a tree object

```{r}
library(phangorn)
#if (!requireNamespace("BiocManager", quietly=TRUE))
#    install.packages("BiocManager")
#BiocManager::install("DECIPHER")
library(DECIPHER)
seqs <- getSequences(seqtab)
names(seqs) <- seqs # This propagates to the tip labels of the tree
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA)

phang.align <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phang.align)
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit = pml(treeNJ, data=phang.align)

## negative edges length changed to 0!

fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
                      rearrangement = "stochastic", control = pml.control(trace = 0))
detach("package:phangorn", unload=TRUE)
```

# Making tidy file - phyloseq object

```{r}


samples.out <- rownames(seqtab.nochim)
subject <- sapply(strsplit(samples.out, "D"), `[`, 1)
gender <- substr(subject,1,1)
subject <- substr(subject,2,999)
day <- as.integer(sapply(strsplit(samples.out, "D"), `[`, 2))
samdf <- data.frame(Subject=subject, Gender=gender, Day=day)
samdf$When <- "Early"
samdf$When[samdf$Day>100] <- "Late"
rownames(samdf) <- samples.out

# The above is your sample data


library(phyloseq)
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa),
               phy_tree(fitGTR$tree))
ps <- prune_samples(sample_names(ps) != "Mock", ps) # Remove mock sample

dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))



ps %>% sample_data()

ps %>% otu_table()

ps %>% tax_table()

ps %>% refseq()

```


# Saving data

```{r}
saveRDS(ps, "phyloseq_example.rds")

sample_data(ps)

otu_table(ps)

```

# Bibliography

```{r warning=FALSE, message=FALSE, echo=FALSE}
#===============================================================================
#BTC.LineZero.Footer.1.1.0
#===============================================================================
#R markdown citation generator.
#===============================================================================
#RLB.Dependencies:
#   magrittr, pacman, stringr
#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
#BTC.Dependencies:
#   LineZero.Header
#===============================================================================
#Generates citations for each explicitly loaded library.
#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
str_libraries <- c("r", str_libraries)
for (str_libraries in str_libraries) {
        str_libraries |>
                pacman::p_citation() |>
                print(bibtex = FALSE) |>
                capture.output() %>%
                .[-1:-3] %>% .[. != ""] |>
                stringr::str_squish() |>
                stringr::str_replace("_", "") |>
                cat()
        cat("\n")
}
#===============================================================================
```